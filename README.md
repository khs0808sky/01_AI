# 01_AI

## 📅 목차

- [2025-08-11](#2025-08-11)
- [2025-08-12](#2025-08-12)

<br><br><br>

---

## **2025-08-11**

---

### 평균제곱 오차(MSE, Mean Squared Error)

* **정의**: 예측값과 실제값의 차이를 제곱한 뒤, 그 평균을 계산한 값.
* **공식**:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

* $y_i$: 실제값
* $\hat{y}_i$: 예측값
* $n$: 데이터 개수
* **특징**

  * 오차를 제곱하므로 양수화되고, 큰 오차에 더 큰 패널티를 부여.
  * 회귀 모델의 성능 지표로 자주 사용됨.
* **단점**

  * 이상치(outlier)에 민감함.

---

### 경사하강법(Gradient Descent)

* **정의**: 비용 함수(cost function)를 최소화하기 위해 매개변수를 반복적으로 조정하는 최적화 알고리즘.
* **아이디어**:

  * 비용 함수의 기울기(gradient)를 계산해, 가장 가파르게 내려가는 방향으로 매개변수를 업데이트.
* **공식**:

$$
\theta = \theta - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta}
$$

* $\theta$: 매개변수(가중치 등)
* $\alpha$: 학습률(learning rate)
* $J(\theta)$: 비용 함수
* **종류**

  1. **배치 경사하강법(Batch GD)**: 모든 데이터 사용 → 안정적, 하지만 느림.
  2. **확률적 경사하강법(SGD)**: 데이터 1개씩 사용 → 빠르지만 변동이 큼.
  3. **미니배치 경사하강법(Mini-batch GD)**: 일정 크기 데이터 묶음 사용 → 속도와 안정성 균형.
* **학습률 주의사항**

  * 너무 크면 발산(oscillation)할 수 있음.
  * 너무 작으면 학습이 느려짐.

---

📅[목차로 돌아가기](#-목차)

---

## **2025-08-12**

---

**퍼셉트론(Perceptron)**

* 인공신경망의 가장 기본적인 형태로, 단일 층에서 입력값에 가중치를 곱하고 합산 후 활성화 함수를 통해 출력을 결정.
* 동작 원리:

  1. 입력 $x_1, x_2, \dots, x_n$과 가중치 $w_1, w_2, \dots, w_n$을 곱해 합산.
  2. 바이어스 $b$를 더함.
  3. 활성화 함수(예: 계단 함수)로 결과를 변환해 출력.
* 단점: XOR 문제처럼 선형 분리가 불가능한 문제는 해결 불가 → 다층 퍼셉트론(MLP) 필요.

---

**오차 역전파(Backpropagation)**

* 다층 신경망에서 학습 시, 출력층에서 계산한 오차를 은닉층으로 거꾸로 전파해 각 가중치를 업데이트하는 알고리즘.
* 과정:

  1. **순전파**: 입력 → 은닉층 → 출력층 계산.
  2. **오차 계산**: 예측값과 실제값 비교 후 손실 함수로 오차 구함.
  3. **역전파**: 오차를 각 층으로 전달하며 기울기 계산(연쇄법칙 이용).
  4. **가중치 업데이트**: 경사하강법 기반으로 조정.
* 장점: 효율적으로 다층 네트워크 학습 가능.

---

**고급 경사하강법(Advanced Gradient Descent)**

* 기본 경사하강법의 한계를 개선한 최적화 기법.
* 주요 기법:

  * **모멘텀(Momentum)**: 이전 기울기를 누적해 관성처럼 적용, 진동 완화 및 빠른 수렴.
  * **AdaGrad**: 각 파라미터별로 학습률 조정, 드문 업데이트가 필요한 파라미터에 큰 학습률 부여.
  * **RMSProp**: 기울기의 제곱 평균을 사용해 학습률 조절, AdaGrad의 학습률 급감 문제 개선.
  * **Adam**: 모멘텀 + RMSProp 결합, 현재 가장 널리 쓰이는 최적화 알고리즘.

---

**원-핫 인코딩(One-Hot Encoding)**

* 범주형 데이터를 컴퓨터가 이해할 수 있는 벡터 형태로 변환하는 방법.
* 원리: N개의 범주 중 해당되는 위치만 1, 나머지는 0으로 표시.
* 예시: {고양이, 강아지, 토끼}에서 "강아지" → \[0, 1, 0].
* 장점: 범주형 변수를 수치화하면서 순서 정보가 포함되지 않음.
* 단점: 범주 수가 많으면 벡터가 매우 커짐(희소 행렬 문제).

---

**학습셋과 테스트셋 구분**

* **학습셋(Training Set)**: 모델을 학습시키는 데 사용되는 데이터.
* **검증셋(Validation Set)**: 하이퍼파라미터 조정 및 과적합 방지에 사용.
* **테스트셋(Test Set)**: 최종적으로 모델의 일반화 성능을 평가하는 데이터.
* 데이터 분리 예시: 일반적으로 6:2:2 또는 8:1:1 비율로 나눔.
* 주의: 테스트셋은 학습 과정에 절대 사용하지 않아야 함(데이터 누수 방지).

---

📅[목차로 돌아가기](#-목차)

---

