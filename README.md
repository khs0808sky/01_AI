# 01_AI

## 📅 목차

- [2025-08-11](#2025-08-11)

<br><br><br>

---

## **2025-08-11**

---

### 평균제곱 오차(MSE, Mean Squared Error)

* **정의**: 예측값과 실제값의 차이를 제곱한 뒤, 그 평균을 계산한 값.
* **공식**:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

* $y_i$: 실제값
* $\hat{y}_i$: 예측값
* $n$: 데이터 개수
* **특징**

  * 오차를 제곱하므로 양수화되고, 큰 오차에 더 큰 패널티를 부여.
  * 회귀 모델의 성능 지표로 자주 사용됨.
* **단점**

  * 이상치(outlier)에 민감함.

---

### 경사하강법(Gradient Descent)

* **정의**: 비용 함수(cost function)를 최소화하기 위해 매개변수를 반복적으로 조정하는 최적화 알고리즘.
* **아이디어**:

  * 비용 함수의 기울기(gradient)를 계산해, 가장 가파르게 내려가는 방향으로 매개변수를 업데이트.
* **공식**:

$$
\theta = \theta - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta}
$$

* $\theta$: 매개변수(가중치 등)
* $\alpha$: 학습률(learning rate)
* $J(\theta)$: 비용 함수
* **종류**

  1. **배치 경사하강법(Batch GD)**: 모든 데이터 사용 → 안정적, 하지만 느림.
  2. **확률적 경사하강법(SGD)**: 데이터 1개씩 사용 → 빠르지만 변동이 큼.
  3. **미니배치 경사하강법(Mini-batch GD)**: 일정 크기 데이터 묶음 사용 → 속도와 안정성 균형.
* **학습률 주의사항**

  * 너무 크면 발산(oscillation)할 수 있음.
  * 너무 작으면 학습이 느려짐.

---

📅[목차로 돌아가기](#-목차)

---
