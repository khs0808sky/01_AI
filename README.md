# 01_AI

## 📅 목차

- [2025-08-11](#2025-08-11)
- [2025-08-12](#2025-08-12)
- [2025-08-13](#2025-08-13)

<br><br><br>

---

## **2025-08-11**

---

### 평균제곱 오차(MSE, Mean Squared Error)

* **정의**: 예측값과 실제값의 차이를 제곱한 뒤, 그 평균을 계산한 값.
* **공식**:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

* $y_i$: 실제값
* $\hat{y}_i$: 예측값
* $n$: 데이터 개수
* **특징**

  * 오차를 제곱하므로 양수화되고, 큰 오차에 더 큰 패널티를 부여.
  * 회귀 모델의 성능 지표로 자주 사용됨.
* **단점**

  * 이상치(outlier)에 민감함.

---

### 경사하강법(Gradient Descent)

* **정의**: 비용 함수(cost function)를 최소화하기 위해 매개변수를 반복적으로 조정하는 최적화 알고리즘.
* **아이디어**:

  * 비용 함수의 기울기(gradient)를 계산해, 가장 가파르게 내려가는 방향으로 매개변수를 업데이트.
* **공식**:

$$
\theta = \theta - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta}
$$

* $\theta$: 매개변수(가중치 등)
* $\alpha$: 학습률(learning rate)
* $J(\theta)$: 비용 함수
* **종류**

  1. **배치 경사하강법(Batch GD)**: 모든 데이터 사용 → 안정적, 하지만 느림.
  2. **확률적 경사하강법(SGD)**: 데이터 1개씩 사용 → 빠르지만 변동이 큼.
  3. **미니배치 경사하강법(Mini-batch GD)**: 일정 크기 데이터 묶음 사용 → 속도와 안정성 균형.
* **학습률 주의사항**

  * 너무 크면 발산(oscillation)할 수 있음.
  * 너무 작으면 학습이 느려짐.

---

📅[목차로 돌아가기](#-목차)

---

## **2025-08-12**

---

**퍼셉트론(Perceptron)**

* 인공신경망의 가장 기본적인 형태로, 단일 층에서 입력값에 가중치를 곱하고 합산 후 활성화 함수를 통해 출력을 결정.
* 동작 원리:

  1. 입력 $x_1, x_2, \dots, x_n$과 가중치 $w_1, w_2, \dots, w_n$을 곱해 합산.
  2. 바이어스 $b$를 더함.
  3. 활성화 함수(예: 계단 함수)로 결과를 변환해 출력.
* 단점: XOR 문제처럼 선형 분리가 불가능한 문제는 해결 불가 → 다층 퍼셉트론(MLP) 필요.

---

**오차 역전파(Backpropagation)**

* 다층 신경망에서 학습 시, 출력층에서 계산한 오차를 은닉층으로 거꾸로 전파해 각 가중치를 업데이트하는 알고리즘.
* 과정:

  1. **순전파**: 입력 → 은닉층 → 출력층 계산.
  2. **오차 계산**: 예측값과 실제값 비교 후 손실 함수로 오차 구함.
  3. **역전파**: 오차를 각 층으로 전달하며 기울기 계산(연쇄법칙 이용).
  4. **가중치 업데이트**: 경사하강법 기반으로 조정.
* 장점: 효율적으로 다층 네트워크 학습 가능.

---

**고급 경사하강법(Advanced Gradient Descent)**

* 기본 경사하강법의 한계를 개선한 최적화 기법.
* 주요 기법:

  * **모멘텀(Momentum)**: 이전 기울기를 누적해 관성처럼 적용, 진동 완화 및 빠른 수렴.
  * **AdaGrad**: 각 파라미터별로 학습률 조정, 드문 업데이트가 필요한 파라미터에 큰 학습률 부여.
  * **RMSProp**: 기울기의 제곱 평균을 사용해 학습률 조절, AdaGrad의 학습률 급감 문제 개선.
  * **Adam**: 모멘텀 + RMSProp 결합, 현재 가장 널리 쓰이는 최적화 알고리즘.

---

**원-핫 인코딩(One-Hot Encoding)**

* 범주형 데이터를 컴퓨터가 이해할 수 있는 벡터 형태로 변환하는 방법.
* 원리: N개의 범주 중 해당되는 위치만 1, 나머지는 0으로 표시.
* 예시: {고양이, 강아지, 토끼}에서 "강아지" → \[0, 1, 0].
* 장점: 범주형 변수를 수치화하면서 순서 정보가 포함되지 않음.
* 단점: 범주 수가 많으면 벡터가 매우 커짐(희소 행렬 문제).

---

**학습셋과 테스트셋 구분**

* **학습셋(Training Set)**: 모델을 학습시키는 데 사용되는 데이터.
* **검증셋(Validation Set)**: 하이퍼파라미터 조정 및 과적합 방지에 사용.
* **테스트셋(Test Set)**: 최종적으로 모델의 일반화 성능을 평가하는 데이터.
* 데이터 분리 예시: 일반적으로 6:2:2 또는 8:1:1 비율로 나눔.
* 주의: 테스트셋은 학습 과정에 절대 사용하지 않아야 함(데이터 누수 방지).

---

📅[목차로 돌아가기](#-목차)

---
---

## **2025-08-13**

---

**데이터의 확인과 검증셋**

* **데이터 확인(Data Inspection)**: 학습 전에 데이터의 분포, 결측치, 이상치 등을 점검.
* **검증셋(Validation Set)**: 학습 과정에서 모델 성능을 평가하고 하이퍼파라미터를 조정하기 위해 사용.

  * 학습셋과 별도로 분리하여 모델이 새로운 데이터에 얼마나 잘 일반화되는지 확인.

---

**과적합(Overfitting)과 자동중단(Early Stopping)**

* **과적합**: 모델이 학습 데이터에 지나치게 맞춰져서 새로운 데이터에 대한 성능이 떨어지는 현상.
* **자동중단(Early Stopping)**: 검증셋 성능이 개선되지 않으면 학습을 조기 종료하여 과적합 방지.

  * 일반적으로 검증 손실이 연속적으로 증가하면 학습 중단.

---

**컨볼루션 신경망(CNN, Convolutional Neural Network)**

* 이미지, 영상 등의 공간적 데이터를 처리하기 위해 설계된 신경망.
* 핵심 요소:

  1. **컨볼루션 레이어**: 필터(커널)를 통해 특징 맵(feature map) 생성.
  2. **맥스 풀링(Max Pooling)**: 공간 크기를 줄이고, 주요 특징만 추출.
  3. **드롭아웃(Dropout)**: 일부 뉴런을 무작위로 비활성화하여 과적합 방지.
  4. **플래튼(Flatten)**: 다차원 데이터를 1차원 벡터로 변환해 완전 연결층(FC)에 입력.

---

**NLP(자연어 처리, Natural Language Processing)**

* 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 기술.
* 주요 작업: 텍스트 분류, 감성 분석, 기계 번역, 챗봇 등.

---

**임베딩(Embedding)**

* 단어, 문장, 아이템 등 범주형 데이터를 실수 벡터로 변환하는 방법.
* 특징:

  * **밀집 표현(Dense Representation)**: 희소 표현(one-hot)보다 효율적.
  * 의미적 유사성을 벡터 거리로 측정 가능.
* 예시:

  * Word2Vec, GloVe: 단어 수준 임베딩.
  * BERT, GPT 계열: 문맥을 반영한 문장 임베딩.

---

📅[목차로 돌아가기](#-목차)

---

